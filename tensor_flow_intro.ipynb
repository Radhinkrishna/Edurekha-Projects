{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_jDcJvVxEPs"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "# Import the required libraries\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(train_dataset, test_dataset), dataset_info = tfds.load('imdb_reviews', split=['train', 'test'], shuffle_files=True, with_info=True, as_supervised=True)\n",
        "\n",
        "# Shuffle the data for training and create batches of text and label pairs\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Encode the text data using TextVectorization layer\n",
        "encoder = TextVectorization(max_tokens=1000)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "# Build a sequential model using tf.keras.Sequential function\n",
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model using train dataset\n",
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)\n",
        "\n",
        "# Test the data using test dataset and evaluate the model by passing a sentence\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)\n",
        "\n",
        "# Predicting sentiment of a sentence\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "\n",
        "if predictions[0] >= 0.0:\n",
        "    print(\"Positive sentiment\")\n",
        "else:\n",
        "    print(\"Negative sentiment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "# Download the flower dataset\n",
        "tf.keras.utils.get_file('flower_photos.tgz', origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz', untar=True)\n",
        "\n",
        "# Analyze the images in the dataset\n",
        "data_dir = './flower_photos'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "validation_dir = os.path.join(data_dir, 'validation')\n",
        "\n",
        "# Get the list of all the images in the training and validation directories\n",
        "train_filenames = tf.io.gfile.glob(os.path.join(train_dir, '*/*.jpg'))\n",
        "validation_filenames = tf.io.gfile.glob(os.path.join(validation_dir, '*/*.jpg'))\n",
        "\n",
        "# Print the number of images in the training and validation datasets\n",
        "print('Number of training images:', len(train_filenames))\n",
        "print('Number of validation images:', len(validation_filenames))\n",
        "# Specify the image resolution\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    image_size=image_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=32,\n",
        "    image_size=image_size,\n",
        "    shuffle=True\n",
        ")\n",
        "# Get the class names\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "# Plot a sample image from each class\n",
        "for i in range(len(class_names)):\n",
        "    plt.figure()\n",
        "    plt.imshow(tf.keras.preprocessing.image.load_img(train_filenames[i]))\n",
        "    plt.title(class_names[i])\n",
        "    plt.show()\n",
        "# Autotune the data\n",
        "train_ds = train_ds.cache().shuffle(1024).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Normalize the data\n",
        "train_ds = train_ds.map(lambda x, y: (x / 255.0, y))\n",
        "validation_ds = validation_ds.map(lambda x, y: (x / 255.0, y))\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(5, activation='softmax')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "VSjvJ5OFy3Ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}